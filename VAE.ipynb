{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BJKb7IMFLFpB_YWGO0OBcLi2J6hESZVY",
      "authorship_tag": "ABX9TyOdzYyQm4o1tzNfB3aYDzJv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaxW-M/Fragment-VAE/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "Uk98dRb2b05d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e7ce7ea-3478-4fe6-fbef-00068db72629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (8.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.3.2\n",
            "tar (child): openbabel-3-1-0.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "/bin/bash: line 0: cd: openbabel-openbabel-3-1-0: No such file or directory\n",
            "python3: can't open file '/content/setup.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#@title Installs\n",
        "\n",
        "!pip install rdkit\n",
        "!tar -xzvf openbabel-3-1-0.tar.gz\n",
        "!cd openbabel-openbabel-3-1-0\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title imports\n",
        "\n",
        "from keras.callbacks import *\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.layers import Input, Conv1D, Embedding, Flatten, Dense, Reshape, Lambda, Activation, Dropout, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from os.path import *\n",
        "import multiprocessing as mp\n",
        "import re\n",
        "import glob\n",
        "#from openbabel import pybel\n",
        "import ast\n",
        "from rdkit import Chem\n",
        "from functools import partial\n",
        "import keras\n",
        "import warnings\n",
        "import unittest\n",
        "from numpy.testing import assert_array_equal\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "disable_eager_execution()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lV2_WVxncMLi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preprocess\n",
        "\n",
        "# the only characters that are allowed to be in a SMILES string for our purposes\n",
        "SMI = ['PAD',\n",
        "       '1', '2', '3', '4', '5', '6', '7', '8', '9', '0',\n",
        "       'c', 'h', 'C', 'H', 'N', 'O', 'P', 'S', 'F', 'C', 'I', 'B',\n",
        "       '/', '-', '(', ')', ',',\n",
        "       'n', 'o', 'p', 's','f', 'c', 'l', 'b', 'r', '.', '=',\n",
        "       '#', '$', ':', '\\\\', '@', '+', '[', ']'  # add'l smi chars\n",
        "       ]\n",
        "\n",
        "\n",
        "'''\n",
        "preconditions: struct is a SMILES string\n",
        "               charset is a list of characters\n",
        "               max_length is the maximum size string allowed for enconding\n",
        "postconditions: returns struct as a vector (an array of ints) or an array of zeros\n",
        "                if output is None\n",
        "'''\n",
        "def struct2vec(struct, charset=SMI, max_length=100):\n",
        "    if pd.isna(struct):\n",
        "      vec = np.array([])\n",
        "    else:\n",
        "      vec = np.zeros(len(struct))\n",
        "      for i in range(len(struct)):\n",
        "        s = struct[i]\n",
        "        if s in charset:\n",
        "            vec[i] = charset.index(s)\n",
        "        else:  # Illegal character in s\n",
        "            return np.zeros(max_length)\n",
        "\n",
        "    if len(vec) > max_length:\n",
        "      return np.zeros(max_length)\n",
        "    else:\n",
        "      vec = np.append(vec, np.zeros(max_length - vec.size))\n",
        "      return np.array(vec, dtype=np.uint8)\n",
        "\n",
        "\n",
        "'''\n",
        "preconditions: smiles is an array of SMILES strings\n",
        "postcondition: converts the SMILES strings to vectors and returns them as an array\n",
        "'''\n",
        "def vectorize(smiles, processes=mp.cpu_count()):\n",
        "    with mp.Pool(processes=processes) as p:\n",
        "        return p.map(struct2vec, smiles)\n",
        "\n",
        "\n",
        "'''\n",
        "preconditions: df is a dataframe with a canonical SMILES column\n",
        "               name is a string\n",
        "               output is an address to an output file\n",
        "               shuffle is a boolean\n",
        "postconditions: adds a vec column to df consisting of its vectorized SMILES column\n",
        "'''\n",
        "def process(df, name, output, shuffle=False):\n",
        "    # shuffle data\n",
        "    if shuffle:\n",
        "        df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # already converted\n",
        "    if 'SMILES' in df.columns:\n",
        "        pass\n",
        "    # error\n",
        "    else:\n",
        "        raise KeyError('Dataframe must have a \"SMILES\" column.')\n",
        "\n",
        "    # turn SMILES strings into vectors\n",
        "    vectors = np.vstack(vectorize(df['SMILES'].values))\n",
        "\n",
        "    # set vectors consisting of only 0s to NaN\n",
        "    vectors = np.where(np.all(vectors == 0, axis=1, keepdims=True), np.nan, vectors)\n",
        "\n",
        "    # add a vec column to df\n",
        "    df['vec'] = vectors.tolist()\n",
        "\n",
        "    # drop any row that has a NaN value in one of its columns\n",
        "    df.dropna(how='any', axis=0, inplace=True)\n",
        "\n",
        "    arr = np.vstack(df['vec'].values)\n",
        "    labels = df.drop(columns=['SMILES', 'vec'])\n",
        "\n",
        "    # save\n",
        "    np.save(os.path.join(output, '%s.npy' % name), arr)\n",
        "\n",
        "    if len(labels.columns) > 0:\n",
        "        np.save(os.path.join(output, '%s_labels.npy' % name), labels.values)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rrtuFJbAcQAl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fragment Preprocessing\n",
        "\n",
        "input_data = ['SMILES', '1-octanol', 'butyl acetate', 'chloroform', 'cyclohexane',\n",
        "              'dichloromethane', 'triolein', 'n-hexane', 'n-octane', 'oleylalcohol',\n",
        "              'toluene', 'n-undecane', 'MONOISOTOPIC_MASS', 'H', 'C', 'N', 'O',\n",
        "              'F', 'P', 'S', 'Cl', 'I', 'Br']\n",
        "\n",
        "'''\n",
        "\n",
        "'i_Al_COO','i_Al_OH','i_Al_OH_noTert','i_ArN',\n",
        "              'i_Ar_COO','i_Ar_N','i_Ar_NH','i_Ar_OH','i_COO','i_COO2','i_C_O','i_C_O_noCOO','i_C_S',\n",
        "              'i_HOCCN','i_Imine','i_NH0','i_NH1','i_NH2','i_N_O','i_Ndealkylation1','i_Ndealkylation2',\n",
        "              'i_Nhpyrrole','i_SH','i_aldehyde','i_alkyl_carbamate','i_alkyl_halide','i_allylic_oxid',\n",
        "              'i_amide','i_amidine','i_aniline','i_aryl_methyl','i_azide','i_azo','i_barbitur','i_benzene',\n",
        "              'i_benzodiazepine','i_bicyclic','i_diazo','i_dihydropyridine','i_epoxide','i_ester','i_ether',\n",
        "              'i_furan','i_guanido','i_halogen','i_hdrzine','i_hdrzone','i_imidazole','i_imide','i_isocyan',\n",
        "              'i_isothiocyan','i_ketone','i_ketone_Topliss','i_lactam','i_lactone','i_methoxy','i_morpholine',\n",
        "              'i_nitrile','i_nitro','i_nitro_arom','i_nitro_arom_nonortho','i_nitroso','i_oxazole','i_oxime',\n",
        "              'i_para_hydroxylation','i_phenol','i_phenol_noOrthoHbond','i_phos_acid','i_phos_ester','i_piperdine',\n",
        "              'i_piperzine','i_priamide','i_prisulfonamd','i_pyridine','i_quatN','i_sulfide','i_sulfonamd','i_sulfone',\n",
        "              'i_term_acetylene','i_tetrazole','i_thiazole','i_thiocyan','i_thiophene','i_unbrch_alkane','i_urea'\n",
        "\n",
        "\n",
        "preconditions: data is the path to the data file\n",
        "               output is the path to the file where the trimmed csv will be saved\n",
        "               set shuffle to True if you want the data shuffled\n",
        "postconditions: returns input and output data as x and y respectively\n",
        "'''\n",
        "def process_fragments(data, output, shuffle=False):\n",
        "    # read csv file with the data\n",
        "    df = pd.read_csv(data)\n",
        "\n",
        "    # remove any non-input or SMILES columns\n",
        "    for col in df.columns:\n",
        "      if col not in input_data:\n",
        "        df.drop(columns=col, inplace=True)\n",
        "\n",
        "    # shuffle the data\n",
        "    if shuffle:\n",
        "      df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # save the trimmed data frame\n",
        "    df.to_csv(output, index=False)\n",
        "\n",
        "    # split the dataframe into x and y values\n",
        "    y_df = df\n",
        "    y_df = y_df[['SMILES']]\n",
        "    df.drop(columns='SMILES', inplace=True)\n",
        "\n",
        "    # turn x into a numpy array\n",
        "    x = df.to_numpy()\n",
        "\n",
        "    return x, y_df"
      ],
      "metadata": {
        "id": "ooVfooX4pMap",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.activations import linear\n",
        "# @title Fragment VAE\n",
        "\n",
        "class fragment_VAE():\n",
        "\n",
        "    def create(self, max_length, nchars, inputdim, **kwargs):\n",
        "      self.max_length = max_length\n",
        "      self.nchars = nchars\n",
        "      self.inputdim = inputdim\n",
        "\n",
        "      self.create_encoder()\n",
        "      self.create_decoder()\n",
        "      self.autoencoder = Model(inputs=self.x,\n",
        "                                 outputs=self.decoder(self.encoder(self.x)),\n",
        "                                 name='vae')\n",
        "\n",
        "      #self.decoder.trainable=False\n",
        "      #self.decoder.trainable = False\n",
        "      opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1E-8, amsgrad=True)\n",
        "      self.autoencoder.compile(optimizer=opt,\n",
        "                                 #loss='mean_squared_error',\n",
        "                                 loss=categorical_crossentropy,\n",
        "                                 metrics=['accuracy', categorical_crossentropy, ignore_accuracy])\n",
        "\n",
        "    def create_encoder(self):\n",
        "      self.x = Input(shape=self.inputdim)\n",
        "      self.encoder = keras.Sequential([Input(shape=self.inputdim),\n",
        "                                       BatchNormalization(),\n",
        "                                       Dropout(0.2),\n",
        "                                       #Dense(292, activation='relu'),\n",
        "                                       #Dropout(0.2),\n",
        "                                       Dense(292, activation='linear')],\n",
        "                                      name='encoder')\n",
        "\n",
        "    def create_decoder(self):\n",
        "        # define latent input\n",
        "        encoded_input = Input(shape=(292,))\n",
        "\n",
        "        # connect to latent dim\n",
        "        h = Reshape((292, 1))(encoded_input)\n",
        "\n",
        "        # build filters\n",
        "        for f, k in zip([10, 10, 11], [9, 9, 10]):\n",
        "            h = Conv1D(f, k, activation='relu', padding='same')(h)\n",
        "\n",
        "        # prepare output dim\n",
        "        h = Flatten()(h)\n",
        "        h = Dense(self.max_length * self.nchars)(h)\n",
        "        h = Reshape((self.max_length, self.nchars))(h)\n",
        "        decoded = Activation('softmax')(h)\n",
        "\n",
        "        # construct decoder (latent->output)\n",
        "        self.decoder = Model(inputs=encoded_input,\n",
        "                             outputs=decoded,\n",
        "                             name='decoder')\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "a custom accuracy metric that ignores padded characters\n",
        "'''\n",
        "\n",
        "def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        "\n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, 0), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "91lmdqARUVI3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fragment Training\n",
        "\n",
        "'''\n",
        "preconditions: data is the path to the input data\n",
        "               output is the path to the output folder\n",
        "               validation is a float that represents the percentage of SMILES strings to be withheld for validation\n",
        "               kernels is a list of ints, the kernel size for each convolution\n",
        "               filters is a list of ints, the number of filters per convolution, must be the same size as kernels\n",
        "               embedding_dim is the input vector of the embedding dimension\n",
        "               latent_dim is the dimension of the latent encoding space\n",
        "               epsilon is the standard deviation of the latent space\n",
        "               freeze_vae freezes the autoencoder weights\n",
        "               weights is the path to a directory containing pretrained weights\n",
        "               patience is the early stopping patience, an int\n",
        "               batch size is the size of training batches, an int\n",
        "               epochs is the number of training epochs\n",
        "\n",
        "postconditions: creates, trains, and saves the model\n",
        "'''\n",
        "def train_fragments(data, output, inputdim=22, validation=0.1, kernels=[9, 9, 10], filters=[10, 10, 11], embedding_dim=32, latent_dim=292,\n",
        "          epsilon=0.1, freeze_vae=False, weights=None, patience=5, batch_size=128, epochs=10):\n",
        "    # load data\n",
        "    x, y_df = process_fragments(data, 'trimmed_data.csv')\n",
        "\n",
        "    process(y_df, 'fragment_y', 'processed_data')\n",
        "    y = np.vstack(y_df['vec'].values).astype(np.uint8)\n",
        "    n, m = y.shape\n",
        "\n",
        "    d = max(np.unique(y)) + 1\n",
        "\n",
        "    # test/train split\n",
        "    mask = test_train_split(x, validation)\n",
        "    x_train = x[mask]\n",
        "    x_validation = x[~mask]\n",
        "    y_train = y[mask]\n",
        "    y_validation = y[~mask]\n",
        "\n",
        "\n",
        "    max_length = m\n",
        "    nchars = d\n",
        "\n",
        "    args = {'which':'train',\n",
        "            'nchars':d,\n",
        "            'max_length':m,\n",
        "            'inputdim':inputdim,\n",
        "            'data':data,\n",
        "            'output':output,\n",
        "            'labels':None,\n",
        "            'kernels':kernels,\n",
        "            'filters':filters,\n",
        "            'embedding_dim':embedding_dim,\n",
        "            'latent_dim':latent_dim,\n",
        "            'epsilon':epsilon,\n",
        "            'dropout':0.2,\n",
        "            'weights':None,\n",
        "            'freeze_vae':False,\n",
        "            'validation':0.1,\n",
        "            'batch_size':batch_size,\n",
        "            'epochs':epochs,\n",
        "            'patience':patience}\n",
        "\n",
        "    savedict(args, output, verbose=False)\n",
        "\n",
        "    #TODO: remove\n",
        "    print(args)\n",
        "\n",
        "    # one-hot encode targets\n",
        "    y_validation = keras.utils.to_categorical(y_validation, d)\n",
        "    y_validation = y_validation.reshape((-1, m, d))\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, d)\n",
        "    y_train = y_train.reshape((-1, m, d))\n",
        "\n",
        "    # initialize autoencoder\n",
        "    model = fragment_VAE()\n",
        "    model.create(max_length, nchars, inputdim)\n",
        "\n",
        "    # model checkpointing\n",
        "    models = [model.autoencoder, model.encoder, model.decoder]\n",
        "    filepaths = [os.path.join(output, f) for f in ('vae.h5',\n",
        "                                                        'encoder.h5',\n",
        "                                                        'decoder.h5',)]\n",
        "    checkpoint = MultiModelCheckpoint(models, filepaths, monitor='val_loss',\n",
        "                                      save_best_only=True, mode='min', save_weights_only=True)\n",
        "\n",
        "    # print model summary\n",
        "    print(model.encoder.summary())\n",
        "    print(model.decoder.summary())\n",
        "    print(model.autoencoder.summary())\n",
        "\n",
        "\n",
        "    # optionally load weights\n",
        "    if weights is not None:\n",
        "        model.encoder_variational.load_weights(os.path.join(weights, 'encoder+v.h5'))\n",
        "        model.decoder.load_weights(os.path.join(weights, 'decoder.h5'))\n",
        "\n",
        "    # early stopping\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
        "\n",
        "    # history\n",
        "    history = LossHistory(os.path.join(output, 'loss_history.tsv'))\n",
        "\n",
        "    # train vae\n",
        "    model.autoencoder.fit(x_train, y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          validation_data=(x_validation, y_validation),\n",
        "                          callbacks=[early_stop, checkpoint, history],\n",
        "                          shuffle=True,\n",
        "                          verbose=2)\n",
        "\n",
        "\n",
        "'''\n",
        "preconditions: x is a numpy array of data\n",
        "               test_size is the percentage of data to be witheld for testing\n",
        "postconditions: returns a random list of ones and zeros that indicate which data points\n",
        "                should be used for testing and which data points should be used for training\n",
        "'''\n",
        "def test_train_split(x, test_size=0.1):\n",
        "\n",
        "    idx = np.random.choice(np.arange(len(x)), size=int(len(x) * test_size), replace=False)\n",
        "    mask = np.ones(len(x)).astype('bool')\n",
        "    mask[idx] = False\n",
        "\n",
        "    return mask\n",
        "\n",
        "'''\n",
        "preconditions: d is a dictionary\n",
        "               path is a path to an output file\n",
        "               verbose is a boolean\n",
        "postconditions: saves d to a file, used to save the variables used in the\n",
        "                creation of a model\n",
        "'''\n",
        "def savedict(d, path, verbose=True):\n",
        "    if verbose:\n",
        "        print('Arguments:')\n",
        "    with open(os.path.join(path, 'arguments.txt'), 'w') as f:\n",
        "        for k, v in d.items():\n",
        "            if v is None:\n",
        "                f.write(\"%s: %s\\n\" % (k, '-1'))\n",
        "            else:\n",
        "                f.write(\"%s: %s\\n\" % (k, v))\n",
        "            if verbose:\n",
        "                print(\"\\t%s: %s\" % (k, v))\n",
        "\n",
        "\n",
        "class MultiModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch.\n",
        "    `filepath` can contain named formatting options,\n",
        "    which will be filled the value of `epoch` and\n",
        "    keys in `logs` (passed in `on_epoch_end`).\n",
        "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
        "    then the model checkpoints will be saved with the epoch number and\n",
        "    the validation loss in the filename.\n",
        "    # Arguments\n",
        "        filepath: string, path to save the model file.\n",
        "        monitor: quantity to monitor.\n",
        "        verbose: verbosity mode, 0 or 1.\n",
        "        save_best_only: if `save_best_only=True`,\n",
        "            the latest best model according to\n",
        "            the quantity monitored will not be overwritten.\n",
        "        mode: one of {auto, min, max}.\n",
        "            If `save_best_only=True`, the decision\n",
        "            to overwrite the current save file is made\n",
        "            based on either the maximization or the\n",
        "            minimization of the monitored quantity. For `val_acc`,\n",
        "            this should be `max`, for `val_loss` this should\n",
        "            be `min`, etc. In `auto` mode, the direction is\n",
        "            automatically inferred from the name of the monitored quantity.\n",
        "        save_weights_only: if True, then only the model's weights will be\n",
        "            saved (`model.save_weights(filepath)`), else the full model\n",
        "            is saved (`model.save(filepath)`).\n",
        "        period: Interval (number of epochs) between checkpoints.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, models, filepaths, monitor='val_loss',\n",
        "                 save_best_only=False, save_weights_only=False,\n",
        "                 mode='auto', period=1):\n",
        "        super(MultiModelCheckpoint, self).__init__()\n",
        "        self.models = models\n",
        "        self.monitor = monitor\n",
        "        self.filepaths = filepaths\n",
        "        self.save_best_only = save_best_only\n",
        "        self.save_weights_only = save_weights_only\n",
        "        self.period = period\n",
        "        self.epochs_since_last_save = 0\n",
        "\n",
        "        if mode not in ['auto', 'min', 'max']:\n",
        "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
        "                          'fallback to auto mode.' % (mode),\n",
        "                          RuntimeWarning)\n",
        "            mode = 'auto'\n",
        "\n",
        "        if mode == 'min':\n",
        "            self.monitor_op = np.less\n",
        "            self.best = np.Inf\n",
        "        elif mode == 'max':\n",
        "            self.monitor_op = np.greater\n",
        "            self.best = -np.Inf\n",
        "        else:\n",
        "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
        "                self.monitor_op = np.greater\n",
        "                self.best = -np.Inf\n",
        "            else:\n",
        "                self.monitor_op = np.less\n",
        "                self.best = np.Inf\n",
        "\n",
        "    '''\n",
        "    preconditions: epoch is the current training epoch\n",
        "                   logs is a log of the monitored values\n",
        "    postconditions: at the end of each epoch if the epoch is a save epoch\n",
        "                    then save the model\n",
        "    '''\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.epochs_since_last_save += 1\n",
        "        if self.epochs_since_last_save >= self.period:\n",
        "            self.epochs_since_last_save = 0\n",
        "            if self.save_best_only:\n",
        "                current = logs.get(self.monitor)\n",
        "                if current is None:\n",
        "                    warnings.warn('Can save best model only with %s available, '\n",
        "                                  'skipping.' % (self.monitor), RuntimeWarning)\n",
        "                else:\n",
        "                    if self.monitor_op(current, self.best):\n",
        "                        self.best = current\n",
        "                        for m, p in zip(self.models, self.filepaths):\n",
        "                            if self.save_weights_only:\n",
        "                                m.save_weights(p, overwrite=True)\n",
        "                            else:\n",
        "                                m.save(p, overwrite=True)\n",
        "            else:\n",
        "                for m, p in zip(self.models, self.filepaths):\n",
        "                    if self.save_weights_only:\n",
        "                        m.save_weights(p, overwrite=True)\n",
        "                    else:\n",
        "                        m.save(p, overwrite=True)\n",
        "\n",
        "'''\n",
        "Saves the loss at the end of each epoch\n",
        "'''\n",
        "class LossHistory(Callback):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.save()\n",
        "\n",
        "    def save(self):\n",
        "        pd.DataFrame(self.losses).to_csv(self.path, index=False, sep='\\t')"
      ],
      "metadata": {
        "id": "Xl_Oy0q2Ub8S"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Predict SMILES from Fragments\n",
        "def predict(data, network):\n",
        "    df = pd.read_csv(data)\n",
        "\n",
        "    name = 'Predicted SMILES'\n",
        "\n",
        "    # split the dataframe into x and y values\n",
        "    df.drop(columns='SMILES', inplace=True)\n",
        "\n",
        "    # load model\n",
        "    config = load_config(join(network, 'arguments.txt'))\n",
        "    config['output'] = network\n",
        "    model = fragment_model_from_config(config)\n",
        "\n",
        "    print(model.encoder.summary())\n",
        "    print(model.decoder.summary())\n",
        "    print(model.autoencoder.summary())\n",
        "\n",
        "\n",
        "    # predict latent\n",
        "    latent = model.encoder.predict(df)\n",
        "\n",
        "    # softmax\n",
        "    softmax = model.decoder.predict(latent)\n",
        "\n",
        "    # argmax and convert to smiles\n",
        "    smiles_out = np.array([vec2struct(x) for x in np.argmax(softmax, axis=-1)])\n",
        "\n",
        "    return smiles_out\n",
        "\n",
        "def fragment_model_from_config(config):\n",
        "    #TODO: Remove\n",
        "    print(config)\n",
        "\n",
        "    # initialize autoencoder\n",
        "    model = fragment_VAE()\n",
        "    model.create(**config)\n",
        "\n",
        "    # load weights\n",
        "    model.encoder.load_weights(os.path.join(config['output'], 'encoder.h5'))\n",
        "    if os.path.exists(os.path.join(config['output'], 'decoder.h5')):\n",
        "        model.decoder.load_weights(os.path.join(config['output'], 'decoder.h5'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_config(filepath):\n",
        "    config = {}\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            #TODO: remove\n",
        "            print(line)\n",
        "\n",
        "            (key, val) = [x.strip() for x in line.split(':')]\n",
        "            if val == '-1':\n",
        "                config[key] = None\n",
        "            elif key in ['nchars', 'max_length', 'embedding_dim', 'nlabels','inputdim',\n",
        "                         'latent_dim', 'batch_size', 'epochs', 'patience', 'seed']:\n",
        "                config[key] = int(val)\n",
        "            elif key in ['kernels', 'filters', 'freeze_vae']:\n",
        "                config[key] = ast.literal_eval(val)\n",
        "            elif key in ['epsilon', 'dropout', 'validation']:\n",
        "                config[key] = float(val)\n",
        "            elif key in ['data', 'output', 'weights', 'labels']:\n",
        "                config[key] = val\n",
        "\n",
        "    config['epsilon_std'] = config.pop('epsilon')\n",
        "    config['output'] = dirname(filepath)\n",
        "    return config\n",
        "\n",
        "def vec2struct(vec, charset=SMI):\n",
        "    '''\n",
        "    Decodes a structure using the given charset.\n",
        "\n",
        "    Parameters\n",
        "    -------\n",
        "    vec : unit8 array\n",
        "        Encoded structure\n",
        "\n",
        "    charset : list, optional\n",
        "        Character set used for encoding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    struct : str\n",
        "        Structure of compound, represented as an InChI or SMILES string.\n",
        "        Note: for InChIs, no layer past the hydrogen layer will be\n",
        "        available. All conformers are possible.\n",
        "    '''\n",
        "\n",
        "    # Init\n",
        "    struct = ''\n",
        "    for i in vec:\n",
        "        try:\n",
        "            # Place character\n",
        "            if charset[i] != 'PAD':\n",
        "                struct += charset[i]\n",
        "        except:\n",
        "            #raise KeyError('Invalid character encountered.')\n",
        "            return None\n",
        "\n",
        "    # Return decoded structure.\n",
        "    return struct\n"
      ],
      "metadata": {
        "id": "f5e0jilNPhjA"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fragment VAE Training\n",
        "train_fragments('drive/MyDrive/Data Sets/TurboChemDB5.2.csv', 'fragment_model', epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C8c1t1LoX7u",
        "outputId": "e574b6ce-763b-49f2-ba31-e059bb59ff5e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'which': 'train', 'nchars': 47, 'max_length': 100, 'inputdim': 22, 'data': 'drive/MyDrive/Data Sets/TurboChemDB5.2.csv', 'output': 'fragment_model', 'labels': None, 'kernels': [9, 9, 10], 'filters': [10, 10, 11], 'embedding_dim': 32, 'latent_dim': 292, 'epsilon': 0.1, 'dropout': 0.2, 'weights': None, 'freeze_vae': False, 'validation': 0.1, 'batch_size': 128, 'epochs': 100, 'patience': 5}\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " batch_normalization_10 (Bat  (None, 22)               88        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 22)                0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 292)               6716      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,804\n",
            "Trainable params: 6,760\n",
            "Non-trainable params: 44\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_32 (InputLayer)       [(None, 292)]             0         \n",
            "                                                                 \n",
            " reshape_17 (Reshape)        (None, 292, 1)            0         \n",
            "                                                                 \n",
            " conv1d_24 (Conv1D)          (None, 292, 10)           100       \n",
            "                                                                 \n",
            " conv1d_25 (Conv1D)          (None, 292, 10)           910       \n",
            "                                                                 \n",
            " conv1d_26 (Conv1D)          (None, 292, 11)           1111      \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 3212)              0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 4700)              15101100  \n",
            "                                                                 \n",
            " reshape_18 (Reshape)        (None, 100, 47)           0         \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 100, 47)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,103,221\n",
            "Trainable params: 15,103,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_30 (InputLayer)       [(None, 22)]              0         \n",
            "                                                                 \n",
            " encoder (Sequential)        (None, 292)               6804      \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 100, 47)           15103221  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,110,025\n",
            "Trainable params: 15,109,981\n",
            "Non-trainable params: 44\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 29181 samples, validate on 3242 samples\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29181/29181 - 122s - loss: 0.7341 - accuracy: 0.8077 - categorical_crossentropy: 0.7341 - ignore_accuracy: 0.4132 - val_loss: 0.5819 - val_accuracy: 0.8199 - val_categorical_crossentropy: 0.5819 - val_ignore_accuracy: 0.4104 - 122s/epoch - 4ms/sample\n",
            "Epoch 2/100\n",
            "29181/29181 - 117s - loss: 0.5772 - accuracy: 0.8181 - categorical_crossentropy: 0.5772 - ignore_accuracy: 0.4212 - val_loss: 0.5519 - val_accuracy: 0.8235 - val_categorical_crossentropy: 0.5519 - val_ignore_accuracy: 0.4259 - 117s/epoch - 4ms/sample\n",
            "Epoch 3/100\n",
            "29181/29181 - 120s - loss: 0.5683 - accuracy: 0.8193 - categorical_crossentropy: 0.5683 - ignore_accuracy: 0.4232 - val_loss: 0.5534 - val_accuracy: 0.8218 - val_categorical_crossentropy: 0.5534 - val_ignore_accuracy: 0.4311 - 120s/epoch - 4ms/sample\n",
            "Epoch 4/100\n",
            "29181/29181 - 117s - loss: 0.5607 - accuracy: 0.8205 - categorical_crossentropy: 0.5607 - ignore_accuracy: 0.4246 - val_loss: 0.5434 - val_accuracy: 0.8240 - val_categorical_crossentropy: 0.5434 - val_ignore_accuracy: 0.4258 - 117s/epoch - 4ms/sample\n",
            "Epoch 5/100\n",
            "29181/29181 - 115s - loss: 0.5552 - accuracy: 0.8214 - categorical_crossentropy: 0.5552 - ignore_accuracy: 0.4253 - val_loss: 0.5431 - val_accuracy: 0.8236 - val_categorical_crossentropy: 0.5431 - val_ignore_accuracy: 0.4267 - 115s/epoch - 4ms/sample\n",
            "Epoch 6/100\n",
            "29181/29181 - 114s - loss: 0.5508 - accuracy: 0.8221 - categorical_crossentropy: 0.5508 - ignore_accuracy: 0.4264 - val_loss: 0.5395 - val_accuracy: 0.8246 - val_categorical_crossentropy: 0.5395 - val_ignore_accuracy: 0.4276 - 114s/epoch - 4ms/sample\n",
            "Epoch 7/100\n",
            "29181/29181 - 116s - loss: 0.5484 - accuracy: 0.8224 - categorical_crossentropy: 0.5484 - ignore_accuracy: 0.4265 - val_loss: 0.5334 - val_accuracy: 0.8259 - val_categorical_crossentropy: 0.5334 - val_ignore_accuracy: 0.4326 - 116s/epoch - 4ms/sample\n",
            "Epoch 8/100\n",
            "29181/29181 - 116s - loss: 0.5459 - accuracy: 0.8231 - categorical_crossentropy: 0.5459 - ignore_accuracy: 0.4284 - val_loss: 0.5330 - val_accuracy: 0.8257 - val_categorical_crossentropy: 0.5330 - val_ignore_accuracy: 0.4312 - 116s/epoch - 4ms/sample\n",
            "Epoch 9/100\n",
            "29181/29181 - 113s - loss: 0.5435 - accuracy: 0.8230 - categorical_crossentropy: 0.5435 - ignore_accuracy: 0.4274 - val_loss: 0.5318 - val_accuracy: 0.8259 - val_categorical_crossentropy: 0.5318 - val_ignore_accuracy: 0.4332 - 113s/epoch - 4ms/sample\n",
            "Epoch 10/100\n",
            "29181/29181 - 117s - loss: 0.5414 - accuracy: 0.8236 - categorical_crossentropy: 0.5414 - ignore_accuracy: 0.4291 - val_loss: 0.5454 - val_accuracy: 0.8218 - val_categorical_crossentropy: 0.5454 - val_ignore_accuracy: 0.4217 - 117s/epoch - 4ms/sample\n",
            "Epoch 11/100\n",
            "29181/29181 - 115s - loss: 0.5403 - accuracy: 0.8238 - categorical_crossentropy: 0.5403 - ignore_accuracy: 0.4296 - val_loss: 0.5298 - val_accuracy: 0.8264 - val_categorical_crossentropy: 0.5298 - val_ignore_accuracy: 0.4333 - 115s/epoch - 4ms/sample\n",
            "Epoch 12/100\n",
            "29181/29181 - 117s - loss: 0.5387 - accuracy: 0.8240 - categorical_crossentropy: 0.5387 - ignore_accuracy: 0.4300 - val_loss: 0.5303 - val_accuracy: 0.8264 - val_categorical_crossentropy: 0.5303 - val_ignore_accuracy: 0.4315 - 117s/epoch - 4ms/sample\n",
            "Epoch 13/100\n",
            "29181/29181 - 115s - loss: 0.5372 - accuracy: 0.8242 - categorical_crossentropy: 0.5372 - ignore_accuracy: 0.4304 - val_loss: 0.5322 - val_accuracy: 0.8257 - val_categorical_crossentropy: 0.5322 - val_ignore_accuracy: 0.4279 - 115s/epoch - 4ms/sample\n",
            "Epoch 14/100\n",
            "29181/29181 - 116s - loss: 0.5352 - accuracy: 0.8246 - categorical_crossentropy: 0.5352 - ignore_accuracy: 0.4310 - val_loss: 0.5269 - val_accuracy: 0.8269 - val_categorical_crossentropy: 0.5269 - val_ignore_accuracy: 0.4313 - 116s/epoch - 4ms/sample\n",
            "Epoch 15/100\n",
            "29181/29181 - 115s - loss: 0.5330 - accuracy: 0.8250 - categorical_crossentropy: 0.5330 - ignore_accuracy: 0.4315 - val_loss: 0.5304 - val_accuracy: 0.8264 - val_categorical_crossentropy: 0.5304 - val_ignore_accuracy: 0.4324 - 115s/epoch - 4ms/sample\n",
            "Epoch 16/100\n",
            "29181/29181 - 117s - loss: 0.5319 - accuracy: 0.8251 - categorical_crossentropy: 0.5319 - ignore_accuracy: 0.4320 - val_loss: 0.5291 - val_accuracy: 0.8268 - val_categorical_crossentropy: 0.5291 - val_ignore_accuracy: 0.4340 - 117s/epoch - 4ms/sample\n",
            "Epoch 17/100\n",
            "29181/29181 - 116s - loss: 0.5313 - accuracy: 0.8252 - categorical_crossentropy: 0.5313 - ignore_accuracy: 0.4323 - val_loss: 0.5324 - val_accuracy: 0.8254 - val_categorical_crossentropy: 0.5324 - val_ignore_accuracy: 0.4289 - 116s/epoch - 4ms/sample\n",
            "Epoch 18/100\n",
            "29181/29181 - 119s - loss: 0.5295 - accuracy: 0.8255 - categorical_crossentropy: 0.5295 - ignore_accuracy: 0.4327 - val_loss: 0.5312 - val_accuracy: 0.8260 - val_categorical_crossentropy: 0.5312 - val_ignore_accuracy: 0.4303 - 119s/epoch - 4ms/sample\n",
            "Epoch 19/100\n",
            "29181/29181 - 117s - loss: 0.5282 - accuracy: 0.8256 - categorical_crossentropy: 0.5282 - ignore_accuracy: 0.4330 - val_loss: 0.5272 - val_accuracy: 0.8268 - val_categorical_crossentropy: 0.5272 - val_ignore_accuracy: 0.4317 - 117s/epoch - 4ms/sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fragment Prediction\n",
        "smiles = predict('trimmed_data.csv', 'fragment_model')\n",
        "smiles_df = pd.DataFrame({'SMILES': smiles})\n",
        "\n",
        "smiles_df.to_csv('%s_smiles.csv' % 'fragment', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1LWDIofLb4x",
        "outputId": "ee6dde0a-d9e0-477d-a060-78c043434fdf"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "which: train\n",
            "\n",
            "nchars: 47\n",
            "\n",
            "max_length: 100\n",
            "\n",
            "inputdim: 22\n",
            "\n",
            "data: drive/MyDrive/Data Sets/TurboChemDB5.2.csv\n",
            "\n",
            "output: fragment_model\n",
            "\n",
            "labels: -1\n",
            "\n",
            "kernels: [9, 9, 10]\n",
            "\n",
            "filters: [10, 10, 11]\n",
            "\n",
            "embedding_dim: 32\n",
            "\n",
            "latent_dim: 292\n",
            "\n",
            "epsilon: 0.1\n",
            "\n",
            "dropout: 0.2\n",
            "\n",
            "weights: -1\n",
            "\n",
            "freeze_vae: False\n",
            "\n",
            "validation: 0.1\n",
            "\n",
            "batch_size: 128\n",
            "\n",
            "epochs: 100\n",
            "\n",
            "patience: 5\n",
            "\n",
            "{'nchars': 47, 'max_length': 100, 'inputdim': 22, 'data': 'drive/MyDrive/Data Sets/TurboChemDB5.2.csv', 'output': 'fragment_model', 'labels': None, 'kernels': [9, 9, 10], 'filters': [10, 10, 11], 'embedding_dim': 32, 'latent_dim': 292, 'dropout': 0.2, 'weights': None, 'freeze_vae': False, 'validation': 0.1, 'batch_size': 128, 'epochs': 100, 'patience': 5, 'epsilon_std': 0.1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization_11 (Bat  (None, 22)               88        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 22)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 292)               6716      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,804\n",
            "Trainable params: 6,760\n",
            "Non-trainable params: 44\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_35 (InputLayer)       [(None, 292)]             0         \n",
            "                                                                 \n",
            " reshape_19 (Reshape)        (None, 292, 1)            0         \n",
            "                                                                 \n",
            " conv1d_27 (Conv1D)          (None, 292, 10)           100       \n",
            "                                                                 \n",
            " conv1d_28 (Conv1D)          (None, 292, 10)           910       \n",
            "                                                                 \n",
            " conv1d_29 (Conv1D)          (None, 292, 11)           1111      \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 3212)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 4700)              15101100  \n",
            "                                                                 \n",
            " reshape_20 (Reshape)        (None, 100, 47)           0         \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 100, 47)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,103,221\n",
            "Trainable params: 15,103,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"vae\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_33 (InputLayer)       [(None, 22)]              0         \n",
            "                                                                 \n",
            " encoder (Sequential)        (None, 292)               6804      \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 100, 47)           15103221  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,110,025\n",
            "Trainable params: 15,109,981\n",
            "Non-trainable params: 44\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        }
      ]
    }
  ]
}